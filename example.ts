// Mock implementation for testing the library structure
// This simulates what the library would do on a mobile device

interface MockContext {
  model: {
    desc: string;
    size: number;
  };
  gpu: boolean;
  reasonNoGPU: string;
  completion: (params: any) => Promise<any>;
  tokenize: (text: string) => Promise<any>;
  embedding: (text: string, params: any) => Promise<any>;
  rerank: (query: string, documents: string[], params: any) => Promise<any>;
  release: () => Promise<void>;
}

// Mock functions
const mockInitLlama = async (params: any): Promise<MockContext> => {
  console.log(' Mock: Initializing Llama with params:', params);
  
  return {
    model: {
      desc: 'Mock Llama 2 7B Chat Model',
      size: 3800000000 // 3.8GB
    },
    gpu: false,
    reasonNoGPU: 'Mock environment - no GPU available',
    completion: async (params: any) => {
      console.log('üîß Mock: Completion called with:', params);
      await new Promise(resolve => setTimeout(resolve, 100)); // Simulate processing time
      
      if (params.messages) {
        // Chat completion
        return {
          content: "This is a mock response from the chat completion. In a real implementation, this would be generated by the Llama model.",
          tokens_predicted: 25,
          timings: { prompt_ms: 50, completion_ms: 100 }
        };
      } else {
        // Text completion
        return {
          text: "This is a mock completion response. The actual Llama model would generate meaningful text based on the prompt.",
          tokens_predicted: 20,
          timings: { prompt_ms: 30, completion_ms: 80 }
        };
      }
    },
    tokenize: async (text: string) => {
      console.log('üîß Mock: Tokenizing text:', text);
      await new Promise(resolve => setTimeout(resolve, 10));
      return {
        tokens: [1, 2, 3, 4, 5], // Mock token IDs
        has_images: false
      };
    },
    embedding: async (text: string, _params: any) => {
      console.log('üîß Mock: Generating embedding for:', text);
      console.log(' Mock: Using params:', _params); // Use the parameter to avoid TS error
      await new Promise(resolve => setTimeout(resolve, 50));
      return {
        embedding: Array.from({ length: 4096 }, () => Math.random() - 0.5) // Mock embedding vector
      };
    },
    rerank: async (query: string, documents: string[], _params: any) => {
      console.log('üîß Mock: Reranking documents for query:', query);
      console.log(' Mock: Using params:', _params); // Use the parameter to avoid TS error
      await new Promise(resolve => setTimeout(resolve, 30));
      return documents.map((doc, index) => ({
        score: Math.random(),
        index,
        document: doc
      })).sort((a, b) => b.score - a.score);
    },
    release: async () => {
      console.log('üîß Mock: Releasing context');
      await new Promise(resolve => setTimeout(resolve, 10));
    }
  };
};

const mockReleaseAllLlama = async (): Promise<void> => {
  console.log('üîß Mock: Releasing all Llama contexts');
  await new Promise(resolve => setTimeout(resolve, 10));
};

const mockToggleNativeLog = async (enabled: boolean): Promise<void> => {
  console.log('üîß Mock: Toggle native log:', enabled);
};

const mockAddNativeLogListener = (_callback: (level: string, text: string) => void) => {
  console.log('üîß Mock: Adding native log listener');
  return {
    remove: () => console.log('üîß Mock: Removed native log listener')
  };
};

const mockBuildInfo = {
  version: '0.0.13',
  buildDate: new Date().toISOString(),
  platform: 'mock'
};

// Test configuration
const TEST_CONFIG = {
  modelPath: './test/models/llama-2-7b-chat.Q4_K_M.gguf',
  contextSize: 2048,
  threads: 4,
  gpuLayers: 0
};

// Test function
async function runMockTest(): Promise<void> {
  console.log(' LlamaCpp Mock Test Suite');
  console.log('='.repeat(50));
  console.log('‚ö†Ô∏è  This is a MOCK test - the actual library requires iOS/Android');
  console.log('Build Info:', mockBuildInfo);

  let context: MockContext | null = null;
  let logListener: any = null;

  try {
    // Setup logging
    await mockToggleNativeLog(true);
    logListener = mockAddNativeLogListener((level: string, text: string) => {
      console.log(`[${level}] ${text}`);
    });

    // Initialize model
    console.log('\n Initializing model...');
    context = await mockInitLlama({
      model: TEST_CONFIG.modelPath,
      n_ctx: TEST_CONFIG.contextSize,
      n_threads: TEST_CONFIG.threads,
      n_gpu_layers: TEST_CONFIG.gpuLayers,
      use_mlock: false,
      use_mmap: true,
      embedding: false,
    });

    console.log('‚úÖ Model loaded successfully!');
    console.log('Model description:', context.model.desc);
    console.log('Model size:', context.model.size);
    console.log('GPU available:', context.gpu);

    // Test 1: Simple completion
    console.log('\nüß™ Test 1: Simple Completion');
    const result1 = await context.completion({
      prompt: "Hello, how are you today?",
      n_predict: 50,
      temperature: 0.8,
      top_p: 0.9,
      top_k: 40,
    });

    console.log('Generated text:', result1.text);
    console.log('Tokens predicted:', result1.tokens_predicted);

    // Test 2: Chat completion
    console.log('\nüß™ Test 2: Chat Completion');
    const result2 = await context.completion({
      messages: [
        { role: "system", content: "You are a helpful AI assistant." },
        { role: "user", content: "What is the capital of France?" }
      ],
      n_predict: 100,
      temperature: 0.7,
    });

    console.log('Chat response:', result2.content);

    // Test 3: Tokenization
    console.log('\nüß™ Test 3: Tokenization');
    const tokenizeResult = await context.tokenize("Hello, world!");
    console.log('Tokens:', tokenizeResult.tokens);
    console.log('Token count:', tokenizeResult.tokens.length);

    // Test 4: Embeddings
    console.log('\n Test 4: Embeddings');
    const embeddingResult = await context.embedding("Hello, world!", {
      embd_normalize: 1.0
    });
    console.log('Embedding length:', embeddingResult.embedding.length);
    console.log('First 5 values:', embeddingResult.embedding.slice(0, 5));

    // Test 5: Reranking
    console.log('\nüß™ Test 5: Reranking');
    const documents = [
      "Document about cats and their behavior",
      "Document about dogs and training methods", 
      "Document about birds and migration patterns"
    ];
    const rerankResults = await context.rerank("Tell me about pets", documents, { normalize: 1.0 });
    console.log('Reranked results:', rerankResults);

    console.log('\n‚úÖ All mock tests completed successfully!');
    console.log('\nüì± To test the real library, you need to:');
    console.log('   1. Build for iOS: npm run build:ios');
    console.log('   2. Build for Android: npm run build:android');
    console.log('   3. Run on a mobile device or simulator');

  } catch (error) {
    console.error('‚ùå Test failed:', error);
    throw error;
  } finally {
    // Cleanup
    console.log('\n Cleaning up...');
    try {
      if (context) {
        await context.release();
      }
      if (logListener) {
        logListener.remove();
      }
      await mockReleaseAllLlama();
      console.log('‚úÖ Cleanup completed');
    } catch (cleanupError) {
      console.error('‚ùå Cleanup failed:', cleanupError);
    }
  }
}

// Error handling
process.on('unhandledRejection', (reason, promise) => {
  console.error('‚ùå Unhandled Rejection at:', promise, 'reason:', reason);
  process.exit(1);
});

// Run the test immediately
console.log('Starting LlamaCpp Mock Test...');
runMockTest().catch((error) => {
  console.error('‚ùå Test execution failed:', error);
  process.exit(1);
});

export { runMockTest };
