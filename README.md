# llama-cpp

A native Capacitor plugin that embeds llama.cpp directly into mobile apps, enabling offline AI inference without requiring external servers.

## Install

```bash
npm install llama-cpp
npx cap sync
```

## API

<docgen-index></docgen-index>

<docgen-api>
<!-- run docgen to generate docs from the source -->
<!-- More info: https://github.com/ionic-team/capacitor-docgen -->
</docgen-api>
